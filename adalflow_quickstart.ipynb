{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacoco/LLM-Finetuning/blob/main/adalflow_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤗 Welcome to AdalFlow!\n",
        "## The Library to Build and to Auto-optimize Any LLM Task Pipeline\n",
        "\n",
        "Thanks for trying us out, we're here to provide you with the best LLM application development experience you can dream of 😊 any questions or concerns you may have, [come talk to us on discord,](https://discord.gg/ezzszrRZvT) we're always here to help!\n",
        "\n",
        "# What is AdalFlow?\n",
        "\n",
        "*AdalFlow* helps developers build and optimize *Retriever-Agent-Generator* pipelines.\n",
        "Embracing similar design pattern to *PyTorch*, AdalFlow is *light*, *modular*, and *robust*, with a 100% readable codebase.\n",
        "\n",
        "# Quick Links\n",
        "\n",
        "Github repo: https://github.com/SylphAI-Inc/AdalFlow\n",
        "\n",
        "Full Tutorials: https://adalflow.sylph.ai/index.html#.\n",
        "\n",
        "\n",
        "# Installation\n",
        "\n",
        "1. Use `pip` to install the `adalflow` Python package. We will need `openai`, `groq`, and `faiss`(cpu version) from the extra packages.\n",
        "\n",
        "  ```bash\n",
        "  pip install adalflow[openai,groq,faiss-cpu]\n",
        "  ```\n",
        "2. Setup  `openai` and `groq` API key in the environment variables"
      ],
      "metadata": {
        "id": "VVSOpjzJl_cx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THTvmhjgfiHE"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install -U adalflow[openai,groq,faiss-cpu]\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Environment Variables\n",
        "\n",
        "Run the following code and pass your api key.\n",
        "\n",
        "Note: for normal `.py` projects, follow our [official installation guide](https://lightrag.sylph.ai/get_started/installation.html).\n",
        "\n",
        "*Go to [OpenAI](https://platform.openai.com/docs/introduction) and [Groq](https://console.groq.com/docs/) to get API keys if you don't already have.*"
      ],
      "metadata": {
        "id": "KapUyHMM07pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt user to enter their API keys securely\n",
        "openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
        "groq_api_key = getpass(\"Please enter your GROQ API key: \")\n",
        "\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "print(\"API keys have been set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONfzF9Puzdd_",
        "outputId": "8272a4e8-6f34-4b68-efc9-8b6d47630981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your OpenAI API key: ··········\n",
            "Please enter your GROQ API key: ··········\n",
            "API keys have been set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 😇 Create Your First ChatBot\n",
        "\n",
        "We will start with a single turn chatbot which will explain concepts with ``explanation`` and ``example``. To achieve this, we will build a simple pipeline to get the **structured output** as ``QAOutput``.\n",
        "\n",
        "\n",
        "##Well-designed Base Classes\n",
        "\n",
        "\n",
        "We will use this use case to demonstrate how to leverage our two and only powerful base classes: `Component` as building blocks for the pipeline and `DataClass` to ease the data interaction with LLMs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SfGS7iddtfpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data and template [jinja2 syntax]\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict\n",
        "import adalflow as adal\n",
        "\n",
        "from adalflow.core import Component, Generator, DataClass, ModelClient\n",
        "from adalflow.components.model_client import GroqAPIClient\n",
        "from adalflow.components.output_parsers import JsonOutputParser\n",
        "\n",
        "@dataclass\n",
        "class QAOutput(DataClass):\n",
        "    explanation: str = field(\n",
        "        metadata={\"desc\": \"A brief explanation of the concept in one sentence.\"}\n",
        "    )\n",
        "    example: str = field(metadata={\"desc\": \"An example of the concept in a sentence.\"})\n",
        "\n",
        "\n",
        "\n",
        "qa_template = r\"\"\"<SYS>\n",
        "You are a helpful assistant.\n",
        "<OUTPUT_FORMAT>\n",
        "{{output_format_str}}\n",
        "</OUTPUT_FORMAT>\n",
        "</SYS>\n",
        "User: {{input_str}}\n",
        "You:\"\"\""
      ],
      "metadata": {
        "id": "kFMtZJcstwtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the task pipeline\n",
        "\n",
        "class QA(adal.Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: Dict):\n",
        "        super().__init__()\n",
        "\n",
        "        parser = JsonOutputParser(data_class=QAOutput, return_data_class=True)\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "            prompt_kwargs={\"output_format_str\": parser.format_instructions()},\n",
        "            output_processors=parser,\n",
        "        )\n",
        "\n",
        "    def call(self, query: str):\n",
        "        return self.generator.call({\"input_str\": query})\n",
        "\n",
        "    async def acall(self, query: str):\n",
        "        return await self.generator.acall({\"input_str\": query})"
      ],
      "metadata": {
        "id": "bWgc1jU0u_jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clear Pipeline Structure\n",
        "\n",
        "Simply by using `print(qa)`, you can see the pipeline structure, which helps users understand any LLM workflow quickly, especially when the pipeline is complicated.\n",
        "\n"
      ],
      "metadata": {
        "id": "9DlqBW1luBN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the QA class\n",
        "\n",
        "qa = QA(\n",
        "    model_client=GroqAPIClient(),\n",
        "    model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        ")\n",
        "\n",
        "print(qa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1_lHozr8jfs",
        "outputId": "f91ed88f-914e-4ae3-97c4-35944905b6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n",
            "QA(\n",
            "  (generator): Generator(\n",
            "    model_kwargs={'model': 'llama3-8b-8192'}, \n",
            "    (prompt): Prompt(\n",
            "      template: <SYS>\n",
            "      You are a helpful assistant.\n",
            "      <OUTPUT_FORMAT>\n",
            "      {{output_format_str}}\n",
            "      </OUTPUT_FORMAT>\n",
            "      </SYS>\n",
            "      User: {{input_str}}\n",
            "      You:, prompt_kwargs: {'output_format_str': 'Your output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\\n    \"example\": \"An example of the concept in a sentence. (str) (required)\"\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.'}, prompt_variables: ['output_format_str', 'input_str']\n",
            "    )\n",
            "    (model_client): GroqAPIClient()\n",
            "    (output_processors): JsonOutputParser(\n",
            "      data_class=QAOutput, examples=None, exclude_fields=None,             include_fields=None, return_data_class=True\n",
            "      (output_format_prompt): Prompt(\n",
            "        template: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "        ```\n",
            "        {{schema}}\n",
            "        ```\n",
            "        {% if example %}\n",
            "        Examples:\n",
            "        ```\n",
            "        {{example}}\n",
            "        ```\n",
            "        {% endif %}\n",
            "        -Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "        -Use double quotes for the keys and string values.\n",
            "        -DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "        -Follow the JSON formatting conventions., prompt_variables: ['schema', 'example']\n",
            "      )\n",
            "      (output_processors): JsonParser()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call the qa and check the output\n",
        "\n",
        "qa(\"What is LLM?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1ly8QgjGBB6",
        "outputId": "01c80bac-732c-488a-abf0-295815337a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data=QAOutput(explanation='LLM stands for Large Language Model, a type of AI model designed to process and generate large amounts of natural language data.', example='For instance, LLMs are used in chatbots and virtual assistants to understand and respond to user queries.'), error=None, usage=CompletionUsage(completion_tokens=63, prompt_tokens=172, total_tokens=235), raw_response='```\\n{\\n    \"explanation\": \"LLM stands for Large Language Model, a type of AI model designed to process and generate large amounts of natural language data.\",\\n    \"example\": \"For instance, LLMs are used in chatbots and virtual assistants to understand and respond to user queries.\"\\n}', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display the prompt only\n",
        "\n",
        "qa.generator.print_prompt(\n",
        "        output_format_str=qa.generator.output_processors.format_instructions(),\n",
        "        input_str=\"What is LLM?\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "o0Np93NtGGy2",
        "outputId": "a16bcb22-e19d-4b83-f1e3-7c657ebca52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "______________________\n",
            "<SYS>\n",
            "You are a helpful assistant.\n",
            "<OUTPUT_FORMAT>\n",
            "Your output should be formatted as a standard JSON instance with the following schema:\n",
            "```\n",
            "{\n",
            "    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\n",
            "    \"example\": \"An example of the concept in a sentence. (str) (required)\"\n",
            "}\n",
            "```\n",
            "-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "-Use double quotes for the keys and string values.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "-Follow the JSON formatting conventions.\n",
            "</OUTPUT_FORMAT>\n",
            "</SYS>\n",
            "User: What is LLM?\n",
            "You:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<SYS>\\nYou are a helpful assistant.\\n<OUTPUT_FORMAT>\\nYour output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\\n    \"example\": \"An example of the concept in a sentence. (str) (required)\"\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.\\n</OUTPUT_FORMAT>\\n</SYS>\\nUser: What is LLM?\\nYou:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-Agnostic\n",
        "\n",
        "You can switch to any model simply by using a different model_client (provider) and model_kwargs.\n",
        "Let's use OpenAI's gpt-3.5-turbo model on the same pipeline."
      ],
      "metadata": {
        "id": "6OYUq0iBVHXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adalflow.components.model_client import OpenAIClient\n",
        "\n",
        "qa_with_gpt = QA(\n",
        "    model_client=OpenAIClient(),\n",
        "    model_kwargs={\"model\": \"gpt-3.5-turbo\"}\n",
        ")\n",
        "\n",
        "qa_with_gpt(\"What is LLM?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fqlYq8sGYKw",
        "outputId": "034cba09-4bbc-4fcd-919e-1e4f0cd4ee33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_OpenAIClient_gpt-3.5-turbo.db\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data=QAOutput(explanation='LLM stands for Large Language Model, which refers to a type of natural language processing model with a high number of parameters.', example='GPT-3 is an example of an LLM that has 175 billion parameters.'), error=None, usage=CompletionUsage(completion_tokens=59, prompt_tokens=169, total_tokens=228), raw_response='```\\n{\\n    \"explanation\": \"LLM stands for Large Language Model, which refers to a type of natural language processing model with a high number of parameters.\",\\n    \"example\": \"GPT-3 is an example of an LLM that has 175 billion parameters.\"\\n}\\n```', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤗 Create First Retrieval-augmented Generation(RAG) pipeline\n",
        "\n",
        "We will use local data base `LocalDB` and `core.data_process` to create a data processing pipeline. This data pipeline will split documents into chunks and work with `LocalDB` to persis the transformed/processed documents in local file `index.faiss` (pickle format)."
      ],
      "metadata": {
        "id": "aVOZV19irzpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Config\n",
        "\n",
        "We will put all configurations together in `config` as dict."
      ],
      "metadata": {
        "id": "WrUmNr8dVh1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs = {\n",
        "    \"embedder\": {\n",
        "        \"batch_size\": 100,\n",
        "        \"model_kwargs\": {\n",
        "            \"model\": \"text-embedding-3-small\",\n",
        "            \"dimensions\": 256,\n",
        "            \"encoding_format\": \"float\",\n",
        "        },\n",
        "    },\n",
        "    \"retriever\": {\n",
        "        \"top_k\": 2,\n",
        "    },\n",
        "    \"generator\": {\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"temperature\": 0.3,\n",
        "        \"stream\": False,\n",
        "    },\n",
        "    \"text_splitter\": {\n",
        "        \"split_by\": \"word\",\n",
        "        \"chunk_size\": 400,\n",
        "        \"chunk_overlap\": 200,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "G2Kpc4d6Vq2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data pipeline\n",
        "\n",
        "Data pipeline requires a sequence of `Document` as inputs."
      ],
      "metadata": {
        "id": "vu1WsDy0Vtdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adalflow.components.data_process import (\n",
        "    RetrieverOutputToContextStr,\n",
        "    ToEmbeddings,\n",
        "    TextSplitter,\n",
        ")\n",
        "from adalflow.core.container import Sequential\n",
        "\n",
        "from adalflow.core.types import Document, ModelClientType\n",
        "\n",
        "\n",
        "def prepare_data_pipeline():\n",
        "    splitter = TextSplitter(**configs[\"text_splitter\"])\n",
        "    embedder = adal.Embedder(\n",
        "        model_client=ModelClientType.OPENAI(),\n",
        "        model_kwargs=configs[\"embedder\"][\"model_kwargs\"],\n",
        "    )\n",
        "    embedder_transformer = ToEmbeddings(\n",
        "        embedder=embedder, batch_size=configs[\"embedder\"][\"batch_size\"]\n",
        "    )\n",
        "    data_transformer = Sequential(splitter, embedder_transformer)\n",
        "    return data_transformer"
      ],
      "metadata": {
        "id": "aOklnBPqVxb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformer = prepare_data_pipeline()\n",
        "data_transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E24GznkJV1-q",
        "outputId": "a03d039e-5012-4739-d0e0-48c32d62b146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): TextSplitter(split_by=word, chunk_size=400, chunk_overlap=200)\n",
              "  (1): ToEmbeddings(\n",
              "    batch_size=100\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "    (batch_embedder): BatchEmbedder(\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare documents for data transformer\n",
        "\n",
        "doc1 = Document(\n",
        "        meta_data={\"title\": \"Li Yin's profile\"},\n",
        "        text=\"My name is Li Yin, I love rock climbing\" + \"lots of nonsense text\" * 500,\n",
        "        id=\"doc1\",\n",
        ")\n",
        "doc2 = Document(\n",
        "    meta_data={\"title\": \"Interviewing Li Yin\"},\n",
        "    text=\"lots of more nonsense text\" * 250\n",
        "    + \"Li Yin is an AI researcher and a software engineer\"\n",
        "    + \"lots of more nonsense text\" * 250,\n",
        "    id=\"doc2\",\n",
        ")\n",
        "\n",
        "doc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMr8U1zhWR8P",
        "outputId": "1c3970a2-9838-43a2-a725-20fdc5ac19af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id=doc1, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector=[], parent_doc_id=None, order=None, score=None)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the data\n",
        "\n",
        "transformed_documents = data_transformer([doc1, doc2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBy8oIkLWtHQ",
        "outputId": "67bec5e9-d163-458f-f77b-44f3ed59eaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting Documents in Batches: 100%|██████████| 1/1 [00:00<00:00, 106.81it/s]\n",
            "Batch embedding documents: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
            "Adding embeddings to documents from batch: 1it [00:00, 6842.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformed documents\n",
        "\n",
        "From the following visualization, we will see `doc1` is splitted into 7 chunks and `doc2` is splitted into 10 chunks. We get this relation from reading the `transformed_documents`, the `parent_doc_id` field.\n",
        "\n",
        "Note: For `text` and `vector`, we dont show the full text or the full vector as it is rather long. You can access each field directly to visualize the full value"
      ],
      "metadata": {
        "id": "I_7meYA9W_Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the transformed data\n",
        "transformed_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lIqy3OqW5HJ",
        "outputId": "092566cc-4a1d-47b1-a36e-dc7f2c2a78ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id=4ecdc0c6-9786-4634-b6dd-4c0d41eb1e0a, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None),\n",
              " Document(id=19db21df-9565-4be8-8073-a935d493b20c, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None),\n",
              " Document(id=1561f534-59c6-4dc8-9da9-3a4304940e6a, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None),\n",
              " Document(id=f6eb5833-51cf-4290-b764-b230decad3fb, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None),\n",
              " Document(id=0ca489b1-d63e-4485-8432-fcb8f00fcb6d, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None),\n",
              " Document(id=c78be750-a64c-4546-8a00-436c8156b50a, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None),\n",
              " Document(id=c198f481-8314-4230-83e8-1ec3832fa47d, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None),\n",
              " Document(id=a037eac3-1df8-41cf-9d02-76f6ea76a1e8, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None),\n",
              " Document(id=9379fbf7-a7d8-46b0-8994-76d6deebd733, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None),\n",
              " Document(id=3c70a850-b329-40ca-861c-773dbf5f5fd3, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None),\n",
              " Document(id=396faed5-1db7-4bc7-9562-ada42f6a8b13, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None),\n",
              " Document(id=9c686dd6-a9e9-460f-800f-a5968bc63413, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None),\n",
              " Document(id=e1214488-6288-4306-904d-d639c365025d, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None),\n",
              " Document(id=4c10b520-cbf4-4f79-8ad8-2a227d735d89, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None),\n",
              " Document(id=d3c9d9a2-5089-4b6d-8a2a-f4f382c0149e, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None),\n",
              " Document(id=9226865f-7223-4edc-86b6-80790fb2983a, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None),\n",
              " Document(id=57518ac8-c2d6-413b-88b6-bbdec00c74a4, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use LocalDB\n",
        "\n",
        "We will use localdb to manage the `documents`, `transformers`, and the persistance of the transformed documents. This resembles more of the production environment where the embeddings and documents are often handled in data base and can be reused to save cost."
      ],
      "metadata": {
        "id": "uKSZmRxeX5t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from adalflow.core.db import LocalDB\n",
        "\n",
        "\n",
        "def prepare_database_with_index(docs: List[Document], index_path: str = \"index.faiss\"):\n",
        "    if os.path.exists(index_path):\n",
        "        return None\n",
        "    db = LocalDB()\n",
        "    db.load(docs)\n",
        "    data_transformer = prepare_data_pipeline()\n",
        "    db.transform(data_transformer, key=\"data_transformer\")\n",
        "    # store\n",
        "    db.save_state(index_path)\n",
        "    print(db)"
      ],
      "metadata": {
        "id": "fhVKVN7MYVVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the database for retriever\n",
        "\n",
        "prepare_database_with_index([doc1, doc2], index_path=\"index.faiss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fviYAerVYjL_",
        "outputId": "241ab5c7-f17f-47eb-e1b9-31813e683645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting Documents in Batches: 100%|██████████| 1/1 [00:00<00:00, 100.18it/s]\n",
            "Batch embedding documents: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
            "Adding embeddings to documents from batch: 1it [00:00, 6482.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalDB(name='LocalDB', items=[Document(id=doc1, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector=[], parent_doc_id=None, order=None, score=None), Document(id=doc2, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector=[], parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=4c1fa2e1-581d-4b57-bbb6-c8f5d5d4da4d, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=55ea86ea-175b-4a6b-ac4e-df303cb8440f, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None), Document(id=464775cb-637a-43bc-be85-908a7a35e9c4, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None), Document(id=0447254c-a093-4e56-9ad1-a7a8730742d0, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None), Document(id=e61b5b84-6309-484b-84fc-8fcb1b1242fa, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None), Document(id=8769e191-b63a-4467-a318-dc1bd74831c6, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None), Document(id=1a0a5161-29b3-488c-aada-86fbf6f2df48, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None), Document(id=7eb96fc2-116b-4c62-99f5-f123a18e2d4b, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None), Document(id=16bb3433-b630-49f5-871d-6142fe67d8e6, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None), Document(id=6c2c4ef5-c4e9-4a00-b890-c1d4626af208, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None), Document(id=657ec50b-467d-4ab4-9d62-c9ee07e3316f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None), Document(id=090645e0-c346-428a-8419-ca21269746dc, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None), Document(id=f5f7f96c-083e-498f-ba96-136b05d4f26e, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None), Document(id=6ff3c50e-df07-4833-9008-ee4f0946b290, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None), Document(id=4d4cfd5e-42dc-4598-822c-6e6f814ad37e, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None), Document(id=e640cea8-a486-4229-8791-acf50665ceb7, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None), Document(id=77513716-2dae-45db-9f60-7b5dd0f0d095, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
            "  (0): TextSplitter(split_by=word, chunk_size=400, chunk_overlap=200)\n",
            "  (1): ToEmbeddings(\n",
            "    batch_size=100\n",
            "    (embedder): Embedder(\n",
            "      model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
            "      (model_client): OpenAIClient()\n",
            "    )\n",
            "    (batch_embedder): BatchEmbedder(\n",
            "      (embedder): Embedder(\n",
            "        model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
            "        (model_client): OpenAIClient()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")}, mapper_setups={})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load from file\n",
        "\n",
        "LocalDB `save_state` not only persist the transformed documents, but also the `data_transformer`.\n",
        "\n",
        "This is really helpful as your retriever needs to have a matching `embedder` to embed the string query. Saving the transformer lets you verify and know what embedder you need to pass to Retriever."
      ],
      "metadata": {
        "id": "a-97wL5gZZKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the database loading\n",
        "\n",
        "db = LocalDB.load_state(\"index.faiss\")\n",
        "db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmKZnA-YZNa1",
        "outputId": "6ef970ec-3753-4a29-b350-04166103ce2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LocalDB(name='LocalDB', items=[Document(id=doc1, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector=[], parent_doc_id=None, order=None, score=None), Document(id=doc2, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector=[], parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=4c1fa2e1-581d-4b57-bbb6-c8f5d5d4da4d, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=55ea86ea-175b-4a6b-ac4e-df303cb8440f, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None), Document(id=464775cb-637a-43bc-be85-908a7a35e9c4, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None), Document(id=0447254c-a093-4e56-9ad1-a7a8730742d0, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None), Document(id=e61b5b84-6309-484b-84fc-8fcb1b1242fa, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None), Document(id=8769e191-b63a-4467-a318-dc1bd74831c6, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None), Document(id=1a0a5161-29b3-488c-aada-86fbf6f2df48, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None), Document(id=7eb96fc2-116b-4c62-99f5-f123a18e2d4b, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None), Document(id=16bb3433-b630-49f5-871d-6142fe67d8e6, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None), Document(id=6c2c4ef5-c4e9-4a00-b890-c1d4626af208, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None), Document(id=657ec50b-467d-4ab4-9d62-c9ee07e3316f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None), Document(id=090645e0-c346-428a-8419-ca21269746dc, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None), Document(id=f5f7f96c-083e-498f-ba96-136b05d4f26e, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None), Document(id=6ff3c50e-df07-4833-9008-ee4f0946b290, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None), Document(id=4d4cfd5e-42dc-4598-822c-6e6f814ad37e, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None), Document(id=e640cea8-a486-4229-8791-acf50665ceb7, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None), Document(id=77513716-2dae-45db-9f60-7b5dd0f0d095, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
              "  (0): TextSplitter(split_by=word, chunk_size=400, chunk_overlap=200)\n",
              "  (1): ToEmbeddings(\n",
              "    batch_size=100\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "    (batch_embedder): BatchEmbedder(\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")}, mapper_setups={})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test data fetching from local db\n",
        "\n",
        "db.get_transformed_data(\"data_transformer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymjXtUQ3axjw",
        "outputId": "51d5b996-eb62-4973-f9b3-02564a552ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id=4c1fa2e1-581d-4b57-bbb6-c8f5d5d4da4d, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None),\n",
              " Document(id=55ea86ea-175b-4a6b-ac4e-df303cb8440f, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None),\n",
              " Document(id=464775cb-637a-43bc-be85-908a7a35e9c4, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None),\n",
              " Document(id=0447254c-a093-4e56-9ad1-a7a8730742d0, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None),\n",
              " Document(id=e61b5b84-6309-484b-84fc-8fcb1b1242fa, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None),\n",
              " Document(id=8769e191-b63a-4467-a318-dc1bd74831c6, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None),\n",
              " Document(id=1a0a5161-29b3-488c-aada-86fbf6f2df48, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None),\n",
              " Document(id=7eb96fc2-116b-4c62-99f5-f123a18e2d4b, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None),\n",
              " Document(id=16bb3433-b630-49f5-871d-6142fe67d8e6, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None),\n",
              " Document(id=6c2c4ef5-c4e9-4a00-b890-c1d4626af208, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None),\n",
              " Document(id=657ec50b-467d-4ab4-9d62-c9ee07e3316f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None),\n",
              " Document(id=090645e0-c346-428a-8419-ca21269746dc, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None),\n",
              " Document(id=f5f7f96c-083e-498f-ba96-136b05d4f26e, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None),\n",
              " Document(id=6ff3c50e-df07-4833-9008-ee4f0946b290, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None),\n",
              " Document(id=4d4cfd5e-42dc-4598-822c-6e6f814ad37e, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None),\n",
              " Document(id=e640cea8-a486-4229-8791-acf50665ceb7, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None),\n",
              " Document(id=77513716-2dae-45db-9f60-7b5dd0f0d095, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG pipeline\n",
        "\n",
        "Now, we will create a RAG pipeline, it consists of:\n",
        "* db (we will load from index_path), we will use `data_transformer` as the key to load the transformed documents.\n",
        "* `FAISSRetriever` which will use embeddings to perform semantic search, and return similarity score in range [0, 1].\n",
        "* `RetrieverOutputToContextStr`: this will convert the retrieved documents to a single str.\n",
        "* `Generator`: we will use a simple `JsonParser` to output a dict with field `answer`."
      ],
      "metadata": {
        "id": "Ve655cL3Y1cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Any\n",
        "\n",
        "from adalflow.core.string_parser import JsonParser\n",
        "from adalflow.components.retriever.faiss_retriever import FAISSRetriever\n",
        "\n",
        "\n",
        "rag_prompt_task_desc = r\"\"\"\n",
        "You are a helpful assistant.\n",
        "\n",
        "Your task is to answer the query that may or may not come with context information.\n",
        "When context is provided, you should stick to the context and less on your prior knowledge to answer the query.\n",
        "\n",
        "Output JSON format:\n",
        "{\n",
        "    \"answer\": \"The answer to the query\",\n",
        "}\"\"\"\n",
        "\n",
        "\n",
        "class RAG(Component):\n",
        "\n",
        "    def __init__(self, index_path: str = \"index.faiss\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.db = LocalDB.load_state(index_path)\n",
        "\n",
        "        self.transformed_docs: List[Document] = self.db.get_transformed_data(\n",
        "            \"data_transformer\"\n",
        "        )\n",
        "        embedder = adal.Embedder(\n",
        "            model_client=ModelClientType.OPENAI(),\n",
        "            model_kwargs=configs[\"embedder\"][\"model_kwargs\"],\n",
        "        )\n",
        "        # map the documents to embeddings\n",
        "        self.retriever = FAISSRetriever(\n",
        "            **configs[\"retriever\"],\n",
        "            embedder=embedder,\n",
        "            documents=self.transformed_docs,\n",
        "            document_map_func=lambda doc: doc.vector,\n",
        "        )\n",
        "        self.retriever_output_processors = RetrieverOutputToContextStr(deduplicate=True)\n",
        "\n",
        "        self.generator = adal.Generator(\n",
        "            prompt_kwargs={\n",
        "                \"task_desc_str\": rag_prompt_task_desc,\n",
        "            },\n",
        "            model_client=OpenAIClient(),\n",
        "            model_kwargs=configs[\"generator\"],\n",
        "            output_processors=JsonParser(),\n",
        "        )\n",
        "\n",
        "    def generate(self, query: str, context: Optional[str] = None) -> Any:\n",
        "        if not self.generator:\n",
        "            raise ValueError(\"Generator is not set\")\n",
        "\n",
        "        prompt_kwargs = {\n",
        "            \"context_str\": context,\n",
        "            \"input_str\": query,\n",
        "        }\n",
        "        response = self.generator(prompt_kwargs=prompt_kwargs)\n",
        "        return response\n",
        "\n",
        "    def call(self, query: str) -> Any:\n",
        "        retrieved_documents = self.retriever(query)\n",
        "        # fill in the document\n",
        "        for i, retriever_output in enumerate(retrieved_documents):\n",
        "            retrieved_documents[i].documents = [\n",
        "                self.transformed_docs[doc_index]\n",
        "                for doc_index in retriever_output.doc_indices\n",
        "            ]\n",
        "\n",
        "        print(f\"retrieved_documents: \\n {retrieved_documents}\\n\")\n",
        "        context_str = self.retriever_output_processors(retrieved_documents)\n",
        "\n",
        "        print(f\"context_str: \\n {context_str}\\n\")\n",
        "\n",
        "        return self.generate(query, context=context_str), retrieved_documents"
      ],
      "metadata": {
        "id": "LdoWsK6dbr0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize rag and visualize its structure\n",
        "\n",
        "rag = RAG(index_path=\"index.faiss\")\n",
        "rag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgFlKDDTcEEN",
        "outputId": "d34f11f4-935d-466d-bae9-0ccd70381863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_OpenAIClient_gpt-3.5-turbo.db\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RAG(\n",
              "  (db): LocalDB(name='LocalDB', items=[Document(id=doc1, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector=[], parent_doc_id=None, order=None, score=None), Document(id=doc2, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector=[], parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=4c1fa2e1-581d-4b57-bbb6-c8f5d5d4da4d, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=55ea86ea-175b-4a6b-ac4e-df303cb8440f, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None), Document(id=464775cb-637a-43bc-be85-908a7a35e9c4, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None), Document(id=0447254c-a093-4e56-9ad1-a7a8730742d0, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None), Document(id=e61b5b84-6309-484b-84fc-8fcb1b1242fa, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None), Document(id=8769e191-b63a-4467-a318-dc1bd74831c6, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None), Document(id=1a0a5161-29b3-488c-aada-86fbf6f2df48, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None), Document(id=7eb96fc2-116b-4c62-99f5-f123a18e2d4b, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None), Document(id=16bb3433-b630-49f5-871d-6142fe67d8e6, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None), Document(id=6c2c4ef5-c4e9-4a00-b890-c1d4626af208, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None), Document(id=657ec50b-467d-4ab4-9d62-c9ee07e3316f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None), Document(id=090645e0-c346-428a-8419-ca21269746dc, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None), Document(id=f5f7f96c-083e-498f-ba96-136b05d4f26e, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None), Document(id=6ff3c50e-df07-4833-9008-ee4f0946b290, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None), Document(id=4d4cfd5e-42dc-4598-822c-6e6f814ad37e, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None), Document(id=e640cea8-a486-4229-8791-acf50665ceb7, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None), Document(id=77513716-2dae-45db-9f60-7b5dd0f0d095, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
              "    (0): TextSplitter(split_by=word, chunk_size=400, chunk_overlap=200)\n",
              "    (1): ToEmbeddings(\n",
              "      batch_size=100\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "      (batch_embedder): BatchEmbedder(\n",
              "        (embedder): Embedder(\n",
              "          model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "          (model_client): OpenAIClient()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )}, mapper_setups={})\n",
              "  (retriever): FAISSRetriever(\n",
              "    top_k=2, metric=prob, dimensions=256, total_documents=17\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "  )\n",
              "  (retriever_output_processors): RetrieverOutputToContextStr(deduplicate=True)\n",
              "  (generator): Generator(\n",
              "    model_kwargs={'model': 'gpt-3.5-turbo', 'temperature': 0.3, 'stream': False}, \n",
              "    (prompt): Prompt(\n",
              "      template: <START_OF_SYSTEM_PROMPT>\n",
              "      {# task desc #}\n",
              "      {% if task_desc_str %}\n",
              "      {{task_desc_str}}\n",
              "      {% else %}\n",
              "      You are a helpful assistant.\n",
              "      {% endif %}\n",
              "      {#input format#}\n",
              "      {% if input_format_str %}\n",
              "      <INPUT_FORMAT>\n",
              "      {{input_format_str}}\n",
              "      </INPUT_FORMAT>\n",
              "      {% endif %}\n",
              "      {# output format #}\n",
              "      {% if output_format_str %}\n",
              "      <OUTPUT_FORMAT>\n",
              "      {{output_format_str}}\n",
              "      </OUTPUT_FORMAT>\n",
              "      {% endif %}\n",
              "      {# tools #}\n",
              "      {% if tools_str %}\n",
              "      <TOOLS>\n",
              "      {{tools_str}}\n",
              "      </TOOLS>\n",
              "      {% endif %}\n",
              "      {# example #}\n",
              "      {% if examples_str %}\n",
              "      <EXAMPLES>\n",
              "      {{examples_str}}\n",
              "      </EXAMPLES>\n",
              "      {% endif %}\n",
              "      {# chat history #}\n",
              "      {% if chat_history_str %}\n",
              "      <CHAT_HISTORY>\n",
              "      {{chat_history_str}}\n",
              "      </CHAT_HISTORY>\n",
              "      {% endif %}\n",
              "      {#contex#}\n",
              "      {% if context_str %}\n",
              "      <CONTEXT>\n",
              "      {{context_str}}\n",
              "      </CONTEXT>\n",
              "      {% endif %}\n",
              "      <END_OF_SYSTEM_PROMPT>\n",
              "      <START_OF_USER_PROMPT>\n",
              "      {% if input_str %}\n",
              "      {{input_str}}\n",
              "      {% endif %}\n",
              "      <END_OF_USER_PROMPT>\n",
              "      {# steps #}\n",
              "      {% if steps_str %}\n",
              "      <START_OF_ASSISTANT_STEPS>\n",
              "      {{steps_str}}\n",
              "      <END_OF_ASSISTANT_STEPS>\n",
              "      {% endif %}\n",
              "      , prompt_kwargs: {'task_desc_str': '\\nYou are a helpful assistant.\\n\\nYour task is to answer the query that may or may not come with context information.\\nWhen context is provided, you should stick to the context and less on your prior knowledge to answer the query.\\n\\nOutput JSON format:\\n{\\n    \"answer\": \"The answer to the query\",\\n}'}, prompt_variables: ['examples_str', 'task_desc_str', 'context_str', 'input_format_str', 'output_format_str', 'steps_str', 'input_str', 'chat_history_str', 'tools_str']\n",
              "    )\n",
              "    (model_client): OpenAIClient()\n",
              "    (output_processors): JsonParser()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run RAG end to end\n",
        "\n",
        "query = \"What is Li Yin's hobby and profession?\"\n",
        "\n",
        "response, retrieved_documents = rag.call(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Hr2w1Lcgq6",
        "outputId": "e2bd4df7-7f22-43f7-aa90-8cd17c099ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calling the call method\n",
            "retrieved_documents: \n",
            " [RetrieverOutput(doc_indices=[0, 11], doc_scores=[0.7120000123977661, 0.6650000214576721], query=\"What is Li Yin's hobby and profession?\", documents=[Document(id=4c1fa2e1-581d-4b57-bbb6-c8f5d5d4da4d, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=090645e0-c346-428a-8419-ca21269746dc, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None)])]\n",
            "\n",
            "context_str: \n",
            "  My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of  textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more \n",
            "\n",
            "GeneratorOutput(id=None, data={'answer': \"Li Yin's hobby is rock climbing and profession is AI researcher and software engineer.\"}, error=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=1145, total_tokens=1168), raw_response='{\\n    \"answer\": \"Li Yin\\'s hobby is rock climbing and profession is AI researcher and software engineer.\"\\n}', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the documents from retriever\n",
        "\n",
        "print(retrieved_documents[0].documents)\n",
        "\n",
        "text1, text2= retrieved_documents[0].documents[0].text, retrieved_documents[0].documents[1].text\n",
        "\n",
        "print(\"rock climbing\" in text1, text1)\n",
        "print(\"software engineer\" in text2, text2)\n",
        "\n",
        "\n",
        "# try to manually search software engineer in the printout, you will find the key word that match the retrieval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYS4aUndklVU",
        "outputId": "90ca1b1c-4046-4782-9fc0-123f75a1b8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id=4c1fa2e1-581d-4b57-bbb6-c8f5d5d4da4d, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=090645e0-c346-428a-8419-ca21269746dc, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None)]\n",
            "True My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of \n",
            "True textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Issues and feedback\n",
        "\n",
        "If you encounter any issues, please report them here: [GitHub Issues](https://github.com/SylphAI-Inc/LightRAG/issues).\n",
        "\n",
        "For feedback, you can use either the [GitHub discussions](https://github.com/SylphAI-Inc/LightRAG/discussions) or [Discord](https://discord.gg/ezzszrRZvT)."
      ],
      "metadata": {
        "id": "3Wnvqs3RyI_z"
      }
    }
  ]
}