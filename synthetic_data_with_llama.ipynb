{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a6b58e974314214ae0c150456fbf5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa1d5232c8ea40558bdf0e76d71931b6",
              "IPY_MODEL_c5034259751544cc98a47d41881c4ef7",
              "IPY_MODEL_52f57295e17d4f1eac3414382b8ea868",
              "IPY_MODEL_9eea7430ed5b4eb6afc991b01907ffe8"
            ],
            "layout": "IPY_MODEL_d63322fa0e9e4230a61af88acb06e87e"
          }
        },
        "a9304a0b0eea40cf9c36348a71fc0f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4846e4f8294f480286a03af21e429f2c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ec9740eeb1c04be0abb385c478838181",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "1d3b455ef5814f5688d810a75436e1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_31c374075a89405ab0ecf5c038937c44",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5236b73dada64b88a62b43cb972471ac",
            "value": ""
          }
        },
        "62b5f498bf1945408a2494e6c3d3e4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_3b78336590114197b7e096e3e6157375",
            "style": "IPY_MODEL_455086b9a7d94baaa8b8262b602bf4c9",
            "value": true
          }
        },
        "8e5dc029dbae4697a9633b246627aa5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_50ce366438d84c92a334e00520b46ee2",
            "style": "IPY_MODEL_0be74d9258c74127be8e7489738d81e6",
            "tooltip": ""
          }
        },
        "9cc39898cbb449b2813c7b0ff5121126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc8c6c02cd3146efb1dbc0b387267361",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_565cfc1d49a6427196b594ad463f939f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d63322fa0e9e4230a61af88acb06e87e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4846e4f8294f480286a03af21e429f2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec9740eeb1c04be0abb385c478838181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31c374075a89405ab0ecf5c038937c44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5236b73dada64b88a62b43cb972471ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b78336590114197b7e096e3e6157375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "455086b9a7d94baaa8b8262b602bf4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50ce366438d84c92a334e00520b46ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be74d9258c74127be8e7489738d81e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "bc8c6c02cd3146efb1dbc0b387267361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "565cfc1d49a6427196b594ad463f939f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46cead04e8f44c8b86f30dec80459b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96a7378ac9a24f74b0e3cf639fd9ee97",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6ee0624354f74bc69915002f626c76d7",
            "value": "Connecting..."
          }
        },
        "96a7378ac9a24f74b0e3cf639fd9ee97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ee0624354f74bc69915002f626c76d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa1d5232c8ea40558bdf0e76d71931b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ffd1ab01134b42b055bf7f37b3d668",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a9a8db11a6a349dcbb2ea5b82af916af",
            "value": "Token is valid (permission: fineGrained)."
          }
        },
        "c5034259751544cc98a47d41881c4ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e804ac1a4745b5a6765ed78f8d141d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f892d56bb38f423ea1068066e8fda017",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "52f57295e17d4f1eac3414382b8ea868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06ed9f0defca43a4b23f29e62a9fe844",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d87050a24e154be3a87e76feb87bfc8e",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "9eea7430ed5b4eb6afc991b01907ffe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a29a21df5beb4481ae7375cb318c11db",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_52a5f965c9b54dcc83f57830d2e43a57",
            "value": "Login successful"
          }
        },
        "f6ffd1ab01134b42b055bf7f37b3d668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a8db11a6a349dcbb2ea5b82af916af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46e804ac1a4745b5a6765ed78f8d141d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f892d56bb38f423ea1068066e8fda017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06ed9f0defca43a4b23f29e62a9fe844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d87050a24e154be3a87e76feb87bfc8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a29a21df5beb4481ae7375cb318c11db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52a5f965c9b54dcc83f57830d2e43a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacoco/LLM-Finetuning/blob/main/synthetic_data_with_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶ô ‚öóÔ∏è Synthetic data generation with Llama 3.1 405B and distilabel\n",
        "\n",
        "This notebook shows how to generate synthetic datasets using the new Llama 3.1 models using [distilabel](https://github.com/argilla-io/distilabel), an open-source framework for synthetic data generation.\n",
        "\n",
        "Thanks to the new 3.1 license, you can now build synthetic datasets to fine-tune smaller, more specialized models using the larger 405B and 70B Llama models.\n",
        "\n",
        "Synthetic data generation is a broad topic and there's many exciting developments and libraries coming out in the past months. distilabel enables you to implement end-to-end data generation pipelines, covering different stages and use cases, such as:\n",
        "\n",
        "- [Generating](https://distilabel.argilla.io/latest/components-gallery/tasks/genstruct/) and [evolving instructions](https://distilabel.argilla.io/latest/components-gallery/tasks/evolinstruct/).\n",
        "- [Generating and selecting data](https://distilabel.argilla.io/latest/sections/pipeline_samples/papers/deita/?h=deita) for supervised fine tuning.\n",
        "- Rating responses for [preference tuning](https://distilabel.argilla.io/latest/components-gallery/tasks/ultrafeedback/?h=ultrafeedback) with [LLM-as-a-judge methods](https://distilabel.argilla.io/latest/components-gallery/tasks/prometheuseval/?h=prometheus).\n",
        "\n",
        "In this notebook, you'll learn the basics of distilabel by generating a preference dataset from scratch using Hugging Face Inference Endpoints. Besides Inference Endpoints, distilabel provides many [out-of-the-box options](https://distilabel.argilla.io/latest/components-gallery/llms/) for running LLM inference, from running local models to using inference providers.\n",
        "\n",
        "Let's get started üöÄ\n",
        "## Install distilabel\n",
        "First you need to install distilabel and the inference endpoints dependencies."
      ],
      "metadata": {
        "id": "dCBXqkAbYJO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wKHzsPsFLYo8",
        "outputId": "77ae7e4c-6f51-4afb-c175-43da4ca96a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m597.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m291.8/291.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install distilabel[hf-inference-endpoints] -U -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login Hugging Face Hub\n",
        "\n",
        "You need to login to be able to use Inference Endpoints. You should use a token with enough rights to run Inference endpoints. If you don't have a token, you can generate one [here](https://huggingface.co/settings/tokens)."
      ],
      "metadata": {
        "id": "JvG5SRuTZ6o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "0a6b58e974314214ae0c150456fbf5ff",
            "a9304a0b0eea40cf9c36348a71fc0f95",
            "1d3b455ef5814f5688d810a75436e1a6",
            "62b5f498bf1945408a2494e6c3d3e4cb",
            "8e5dc029dbae4697a9633b246627aa5e",
            "9cc39898cbb449b2813c7b0ff5121126",
            "d63322fa0e9e4230a61af88acb06e87e",
            "4846e4f8294f480286a03af21e429f2c",
            "ec9740eeb1c04be0abb385c478838181",
            "31c374075a89405ab0ecf5c038937c44",
            "5236b73dada64b88a62b43cb972471ac",
            "3b78336590114197b7e096e3e6157375",
            "455086b9a7d94baaa8b8262b602bf4c9",
            "50ce366438d84c92a334e00520b46ee2",
            "0be74d9258c74127be8e7489738d81e6",
            "bc8c6c02cd3146efb1dbc0b387267361",
            "565cfc1d49a6427196b594ad463f939f",
            "46cead04e8f44c8b86f30dec80459b55",
            "96a7378ac9a24f74b0e3cf639fd9ee97",
            "6ee0624354f74bc69915002f626c76d7",
            "aa1d5232c8ea40558bdf0e76d71931b6",
            "c5034259751544cc98a47d41881c4ef7",
            "52f57295e17d4f1eac3414382b8ea868",
            "9eea7430ed5b4eb6afc991b01907ffe8",
            "f6ffd1ab01134b42b055bf7f37b3d668",
            "a9a8db11a6a349dcbb2ea5b82af916af",
            "46e804ac1a4745b5a6765ed78f8d141d",
            "f892d56bb38f423ea1068066e8fda017",
            "06ed9f0defca43a4b23f29e62a9fe844",
            "d87050a24e154be3a87e76feb87bfc8e",
            "a29a21df5beb4481ae7375cb318c11db",
            "52a5f965c9b54dcc83f57830d2e43a57"
          ]
        },
        "id": "ci5Nok9jZ_8G",
        "outputId": "5680cfcd-860e-4478-a463-1906777c31f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a6b58e974314214ae0c150456fbf5ff"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quickstart\n",
        "\n",
        "Let's start with a quick example: a pipeline to build a preference dataset with the following steps:\n",
        "\n",
        "- Load a dataset with instructions from the Hugging Face Hub using the `LoadDataFromHub` [step](https://distilabel.argilla.io/latest/components-gallery/steps/loaddatafromhub/).\n",
        "- For each prompt, generate two responses using the `TextGeneration` task with the `InferenceEndpointsLLM` [LLM](https://distilabel.argilla.io/latest/components-gallery/llms/inferenceendpointsllm/) and the 405B and 70B models.\n",
        "- Combine the two responses into a list of responses using the `CombineColumns` [step](https://distilabel.argilla.io/latest/components-gallery/steps/combinecolumns/).\n",
        "- Compare and rate the responses using the `UltraFeedback` [llm-as-a-judge task](https://distilabel.argilla.io/latest/components-gallery/tasks/ultrafeedback/) with the 405B model.\n",
        "\n",
        "See below the input dataset with instructions. The pipeline will use the `instruction` column to generate responses with Llama 3.1 models."
      ],
      "metadata": {
        "id": "_RWPg572aDIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "iframe_html = \"\"\"\n",
        "<iframe src=\"https://huggingface.co/datasets/argilla/10Kprompts-mini/embed/viewer/train\" width=\"80%\" height=\"560px\"></iframe>\n",
        "\"\"\"\n",
        "display(HTML(iframe_html))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "0PA4cVMSz0NG",
        "outputId": "1110b256-06b3-4ab2-c2c5-b0dd399655cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe src=\"https://huggingface.co/datasets/argilla/10Kprompts-mini/embed/viewer/train\" width=\"80%\" height=\"560px\"></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run the pipeline:"
      ],
      "metadata": {
        "id": "6hE3-qC106lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.llms import InferenceEndpointsLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import LoadDataFromHub\n",
        "from distilabel.steps.tasks import TextGeneration, UltraFeedback\n",
        "from distilabel.steps import CombineColumns\n",
        "\n",
        "llama70B = InferenceEndpointsLLM(\n",
        "    model_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "    tokenizer_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "    generation_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        ")\n",
        "llama405B = InferenceEndpointsLLM(\n",
        "    model_id=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
        "    tokenizer_id=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
        "    generation_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        ")\n",
        "\n",
        "with Pipeline(name=\"synthetic-data-with-llama3\") as pipeline:\n",
        "\n",
        "    # load dataset with prompts\n",
        "    load_dataset = LoadDataFromHub(\n",
        "        repo_id= \"argilla/10Kprompts-mini\"\n",
        "    )\n",
        "\n",
        "    # generate two responses\n",
        "    generate = [\n",
        "        TextGeneration(llm=llama70B),\n",
        "        TextGeneration(llm=llama405B)\n",
        "    ]\n",
        "\n",
        "    # combine responses into one col\n",
        "    combine = CombineColumns(\n",
        "        columns=[\"generation\", \"model_name\"],\n",
        "        output_columns=[\"generations\", \"model_names\"]\n",
        "    )\n",
        "\n",
        "    # rate responses with 405B LLM-as-a-judge\n",
        "    rate = UltraFeedback(aspect=\"overall-rating\", llm=llama405B)\n",
        "\n",
        "    # define and run pipeline\n",
        "    load_dataset >> generate >> combine >> rate"
      ],
      "metadata": {
        "id": "IIa06xswLi_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distiset = pipeline.run(use_cache=False)"
      ],
      "metadata": {
        "id": "eYPLlgDgHNp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distiset['default']['train'].to_pandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bmSW3jXIHVL8",
        "outputId": "ef84dd58-8e3f-4094-ec10-50d951594e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          instruction                   topic  \\\n",
              "0   How can I create an efficient and robust workf...    Software Development   \n",
              "1   Is it possible to convert DC welding machine t...     Literature and Arts   \n",
              "2   Delete a part of the sentence that does not fi...  Science and Technology   \n",
              "3   Construct a daily schedule that allocates exac...     Health and Wellness   \n",
              "4   If a particular argument hinges on an anecdota...                  Others   \n",
              "5   pretend avwave is a monkey from the jungles of...     Literature and Arts   \n",
              "6   Generate sentences that use the idiom ‚Äòonce in...    Software Development   \n",
              "7   You are my coding tutor. You reply with code a...    Software Development   \n",
              "8   Can you explain how the @property decorator is...    Software Development   \n",
              "9   Web search results:\\n\\n[1] \"Positive psycholog...     Health and Wellness   \n",
              "10  Write a fictional news article: A massive alie...     Literature and Arts   \n",
              "11  What are some alternatives to meditation havin...     Health and Wellness   \n",
              "12  I need abstract (up to 300 words) about two ca...     Health and Wellness   \n",
              "13  Please answer the following question.\\n\\nQuest...    Software Development   \n",
              "14  Write a short story about the Chinese spy ball...     Literature and Arts   \n",
              "15  Can you re-write a modern version of the tell ...     Literature and Arts   \n",
              "16  I am a college professor preparing a lecture o...  Science and Technology   \n",
              "17  Evaluate the logical validity of the argument:...                    Math   \n",
              "18  You are given a code snippet that contains a f...    Software Development   \n",
              "19  What strategies have scientists developed for ...     Health and Wellness   \n",
              "\n",
              "                                          generations  \\\n",
              "0   [To create an efficient and robust workflow th...   \n",
              "1   [While it's technically possible to modify a D...   \n",
              "2   [The part of the sentence that doesn't fit the...   \n",
              "3   [Here is a sample daily schedule that allocate...   \n",
              "4   [When an argument relies heavily on anecdotal ...   \n",
              "5   [In the depths of Zealandia's jungle, where th...   \n",
              "6   [Here are a few sentences that use the idiom \"...   \n",
              "7   [I'll provide the necessary code snippets to g...   \n",
              "8   [A nice piece of Python code!\\n\\nThe `@propert...   \n",
              "9   [Here is a comprehensive explanation of positi...   \n",
              "10  [**DATE:** March 15, 2023\\n\\n**NASA CONFIRMS E...   \n",
              "11  [While meditation is an excellent way to culti...   \n",
              "12  [Here is an abstract of two case studies of ch...   \n",
              "13  [A nice question about the subtleties of C++ c...   \n",
              "14  [It was a typical winter morning in Billings, ...   \n",
              "15  [Here's a modern retelling of Edgar Allan Poe'...   \n",
              "16  [What an exciting topic! Here's a possible int...   \n",
              "17  [A nice example of a logical argument! Let's b...   \n",
              "18  [Here is the completed code snippet with the l...   \n",
              "19  [Scientists have developed several strategies ...   \n",
              "\n",
              "                                  distilabel_metadata  \\\n",
              "0   {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "1   {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "2   {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "3   {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "4               {'raw_output_ultra_feedback_0': None}   \n",
              "5   {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "6   {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "7   {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "8               {'raw_output_ultra_feedback_0': None}   \n",
              "9   {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "10  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "11              {'raw_output_ultra_feedback_0': None}   \n",
              "12  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "13  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "14  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "15  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "16  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "17  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "18  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "19  {'raw_output_ultra_feedback_0': '#### Output f...   \n",
              "\n",
              "                                          model_names     ratings  \\\n",
              "0   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 4.0]   \n",
              "1   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 5.0]   \n",
              "2   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [4.0, 5.0]   \n",
              "3   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 5.0]   \n",
              "4   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [nan, nan]   \n",
              "5   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 4.0]   \n",
              "6   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 5.0]   \n",
              "7   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 4.0]   \n",
              "8   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [nan, nan]   \n",
              "9   [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [4.0, 5.0]   \n",
              "10  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 4.0]   \n",
              "11  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [nan, nan]   \n",
              "12  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 4.0]   \n",
              "13  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 4.0]   \n",
              "14  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [4.0, 5.0]   \n",
              "15  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [4.0, 3.0]   \n",
              "16  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 4.0]   \n",
              "17  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 5.0]   \n",
              "18  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 5.0]   \n",
              "19  [meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...  [5.0, 4.0]   \n",
              "\n",
              "                                           rationales  \\\n",
              "0   [The text provides a clear, step-by-step guide...   \n",
              "1   [The text provides an accurate and informative...   \n",
              "2   [This output is near perfect as it accurately ...   \n",
              "3   [This schedule accurately allocates 8 hours fo...   \n",
              "4                                        [None, None]   \n",
              "5   [This text provides an engaging and well-struc...   \n",
              "6   [Text 1 provides accurate and informative sent...   \n",
              "7   [This response provides accurate and helpful i...   \n",
              "8                                        [None, None]   \n",
              "9   [The text provides a comprehensive explanation...   \n",
              "10  [The text provides accurate and informative co...   \n",
              "11                                       [None, None]   \n",
              "12  [This abstract accurately and confidently pres...   \n",
              "13  [The text provides a clear and accurate explan...   \n",
              "14  [The story is engaging, informative, and align...   \n",
              "15  [This text provides a good modern retelling of...   \n",
              "16  [This introduction is excellent in terms of co...   \n",
              "17  [This text provides a thorough and accurate an...   \n",
              "18  [The output accurately completes the code snip...   \n",
              "19  [Text 1 provides a comprehensive and well-stru...   \n",
              "\n",
              "                                     model_name  \n",
              "0   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "1   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "2   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "3   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "4   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "5   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "6   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "7   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "8   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "9   meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "10  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "11  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "12  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "13  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "14  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "15  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "16  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "17  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "18  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  \n",
              "19  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d3d2d47-4555-4d27-b17b-9878120a43d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>topic</th>\n",
              "      <th>generations</th>\n",
              "      <th>distilabel_metadata</th>\n",
              "      <th>model_names</th>\n",
              "      <th>ratings</th>\n",
              "      <th>rationales</th>\n",
              "      <th>model_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How can I create an efficient and robust workf...</td>\n",
              "      <td>Software Development</td>\n",
              "      <td>[To create an efficient and robust workflow th...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 4.0]</td>\n",
              "      <td>[The text provides a clear, step-by-step guide...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is it possible to convert DC welding machine t...</td>\n",
              "      <td>Literature and Arts</td>\n",
              "      <td>[While it's technically possible to modify a D...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 5.0]</td>\n",
              "      <td>[The text provides an accurate and informative...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Delete a part of the sentence that does not fi...</td>\n",
              "      <td>Science and Technology</td>\n",
              "      <td>[The part of the sentence that doesn't fit the...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[4.0, 5.0]</td>\n",
              "      <td>[This output is near perfect as it accurately ...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Construct a daily schedule that allocates exac...</td>\n",
              "      <td>Health and Wellness</td>\n",
              "      <td>[Here is a sample daily schedule that allocate...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 5.0]</td>\n",
              "      <td>[This schedule accurately allocates 8 hours fo...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If a particular argument hinges on an anecdota...</td>\n",
              "      <td>Others</td>\n",
              "      <td>[When an argument relies heavily on anecdotal ...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': None}</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[nan, nan]</td>\n",
              "      <td>[None, None]</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>pretend avwave is a monkey from the jungles of...</td>\n",
              "      <td>Literature and Arts</td>\n",
              "      <td>[In the depths of Zealandia's jungle, where th...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 4.0]</td>\n",
              "      <td>[This text provides an engaging and well-struc...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Generate sentences that use the idiom ‚Äòonce in...</td>\n",
              "      <td>Software Development</td>\n",
              "      <td>[Here are a few sentences that use the idiom \"...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 5.0]</td>\n",
              "      <td>[Text 1 provides accurate and informative sent...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>You are my coding tutor. You reply with code a...</td>\n",
              "      <td>Software Development</td>\n",
              "      <td>[I'll provide the necessary code snippets to g...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 4.0]</td>\n",
              "      <td>[This response provides accurate and helpful i...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Can you explain how the @property decorator is...</td>\n",
              "      <td>Software Development</td>\n",
              "      <td>[A nice piece of Python code!\\n\\nThe `@propert...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': None}</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[nan, nan]</td>\n",
              "      <td>[None, None]</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Web search results:\\n\\n[1] \"Positive psycholog...</td>\n",
              "      <td>Health and Wellness</td>\n",
              "      <td>[Here is a comprehensive explanation of positi...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[4.0, 5.0]</td>\n",
              "      <td>[The text provides a comprehensive explanation...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Write a fictional news article: A massive alie...</td>\n",
              "      <td>Literature and Arts</td>\n",
              "      <td>[**DATE:** March 15, 2023\\n\\n**NASA CONFIRMS E...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 4.0]</td>\n",
              "      <td>[The text provides accurate and informative co...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What are some alternatives to meditation havin...</td>\n",
              "      <td>Health and Wellness</td>\n",
              "      <td>[While meditation is an excellent way to culti...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': None}</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[nan, nan]</td>\n",
              "      <td>[None, None]</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>I need abstract (up to 300 words) about two ca...</td>\n",
              "      <td>Health and Wellness</td>\n",
              "      <td>[Here is an abstract of two case studies of ch...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 4.0]</td>\n",
              "      <td>[This abstract accurately and confidently pres...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Please answer the following question.\\n\\nQuest...</td>\n",
              "      <td>Software Development</td>\n",
              "      <td>[A nice question about the subtleties of C++ c...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 4.0]</td>\n",
              "      <td>[The text provides a clear and accurate explan...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Write a short story about the Chinese spy ball...</td>\n",
              "      <td>Literature and Arts</td>\n",
              "      <td>[It was a typical winter morning in Billings, ...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[4.0, 5.0]</td>\n",
              "      <td>[The story is engaging, informative, and align...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Can you re-write a modern version of the tell ...</td>\n",
              "      <td>Literature and Arts</td>\n",
              "      <td>[Here's a modern retelling of Edgar Allan Poe'...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[4.0, 3.0]</td>\n",
              "      <td>[This text provides a good modern retelling of...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>I am a college professor preparing a lecture o...</td>\n",
              "      <td>Science and Technology</td>\n",
              "      <td>[What an exciting topic! Here's a possible int...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 4.0]</td>\n",
              "      <td>[This introduction is excellent in terms of co...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Evaluate the logical validity of the argument:...</td>\n",
              "      <td>Math</td>\n",
              "      <td>[A nice example of a logical argument! Let's b...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 5.0]</td>\n",
              "      <td>[This text provides a thorough and accurate an...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>You are given a code snippet that contains a f...</td>\n",
              "      <td>Software Development</td>\n",
              "      <td>[Here is the completed code snippet with the l...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 5.0]</td>\n",
              "      <td>[The output accurately completes the code snip...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>What strategies have scientists developed for ...</td>\n",
              "      <td>Health and Wellness</td>\n",
              "      <td>[Scientists have developed several strategies ...</td>\n",
              "      <td>{'raw_output_ultra_feedback_0': '#### Output f...</td>\n",
              "      <td>[meta-llama/Meta-Llama-3.1-70B-Instruct, meta-...</td>\n",
              "      <td>[5.0, 4.0]</td>\n",
              "      <td>[Text 1 provides a comprehensive and well-stru...</td>\n",
              "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-FP8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d3d2d47-4555-4d27-b17b-9878120a43d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d3d2d47-4555-4d27-b17b-9878120a43d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d3d2d47-4555-4d27-b17b-9878120a43d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-00e2658c-3e64-4a99-99a6-60bb57745866\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00e2658c-3e64-4a99-99a6-60bb57745866')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-00e2658c-3e64-4a99-99a6-60bb57745866 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"distiset['default']['train']\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"How can I create an efficient and robust workflow that utilizes advanced automation techniques to extract targeted data, including customer information, from diverse PDF documents and effortlessly integrate it into a designated Google Sheet? Furthermore, I am interested in establishing a comprehensive and seamless system that promptly activates an SMS notification on my mobile device whenever a new PDF document is uploaded to the Google Sheet, ensuring real-time updates and enhanced accessibility.\",\n          \"Evaluate the logical validity of the argument: \\\"If a number is even, then it is divisible by 2. Number 6 is even. Therefore, number 6 is divisible by 2.\\\"\",\n          \"Can you re-write a modern version of the tell tale heart but the heart is a cellphone?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"topic\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Software Development\",\n          \"Literature and Arts\",\n          \"Math\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generations\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"distilabel_metadata\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_names\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ratings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rationales\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distiset['default']['train'][15]"
      ],
      "metadata": {
        "id": "_dDtC7XnuXQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, we can push the dataset to the Hub:"
      ],
      "metadata": {
        "id": "VFYe7ni0glzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# set a secret in colab with enough rights to write repos\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "distiset.push_to_hub(\n",
        "    \"argilla/synthetic-data-generation-with-llama3-405B\",\n",
        "    token=hf_token,\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "mK6JBGvySrAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now explore the resulting dataset below. The most relevant columns are:\n",
        "\n",
        "- `generations`: A list of the two generated responses (70B and 405B), generated in the `generate` step.\n",
        "- `ratings`: A list with a rating for each response, generated by the `rate` step.\n",
        "- `rationales`: A list with rationale for the rating of each response, generated by the `rate` step."
      ],
      "metadata": {
        "id": "Vp79SC6ogryb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "iframe_html = \"\"\"\n",
        "<iframe src=\"https://huggingface.co/datasets/argilla/synthetic-data-generation-with-llama3-405B/embed/viewer/train\" width=\"80%\" height=\"560px\"></iframe>\n",
        "\"\"\"\n",
        "display(HTML(iframe_html))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "cXr4JrwkSDZo",
        "outputId": "2450645b-3b97-417b-e7f0-ad9974b2a96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe src=\"https://huggingface.co/datasets/argilla/synthetic-data-generation-with-llama3-405B/embed/viewer/train\" width=\"80%\" height=\"560px\"></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéâ Congrats! You've generated your first synthetic dataset with distilabel and Llama3.1 405B.\n",
        "\n",
        "The next section covers how to further configure the pipeline and introduces other useful out-of-the-box steps offered by distilabel."
      ],
      "metadata": {
        "id": "Qwhh3kJpHv9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced usage\n",
        "\n",
        "The above example, although simple, is effective and got us a nice dataset but we can try to improve it tweaking the generation parameters of the used LLMs or even combining a few LLMs to generate better texts. Let's see how!\n",
        "\n",
        "### Tweaking the generation parameters\n",
        "\n",
        "We can define new generation parameters for both the models we used to generate texts (`Llama 3.1 70B Instruct` and `Llama 3.1 405B Instruct`) and the model (`Llama 3.1 405B Instruct`) we used to rate those generations using the `parameters` argument of the `run` method:\n",
        "\n",
        "\n",
        "1. For the Llamas that will be used to generate text with the `TextGeneration` task we will define that we want them at max generating `512` tokens with the `max_new_tokens` parameter (default was `128`), and in addition we will set the `temperature` to `0.7` to make the probability distribution of the tokens predicted more uniform or random, so we get more rich and creative texts.\n",
        "2. For the Llama used to rate the generations with `UltraFeedback` task we will set the maximun number of tokens to be generated to `2048`, as the LLM will have to generate a rationale and score for each generation (in this case 2) so we want to be sure that the LLM will have enough tokens to do so. In this case, as we're using an LLM to annotate the generations, we want it to be as deterministic as possible so we will set the `temperature` to `0.1`.\n",
        "\n",
        "In most of the cases, setting the `max_new_tokens` and `temperature` is enough to achieve the results that we want, but we can define [much more parameters](https://distilabel.argilla.io/latest/api/llm/huggingface/#distilabel.llms.huggingface.inference_endpoints.InferenceEndpointsLLM.agenerate) such as the `top_p` and `top_k` to adjust even more the tokens generated.\n",
        "\n"
      ],
      "metadata": {
        "id": "QkZaQ281gvx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters={\n",
        "    # Llama 3.1 70B Instruct used for text generation\n",
        "    generate[0].name: {\n",
        "        \"llm\": {\n",
        "            \"generation_kwargs\": {\n",
        "                \"max_new_tokens\": 512,\n",
        "                \"temperature\": 0.7,\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # Llama 3.1 405B Instruct used for text generation\n",
        "    generate[1].name: {\n",
        "        \"llm\": {\n",
        "            \"generation_kwargs\": {\n",
        "                \"max_new_tokens\": 512,\n",
        "                \"temperature\": 0.7,\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # Llama 3.1 405B Instruct used judging responses\n",
        "    rate.name: {\n",
        "        \"llm\": {\n",
        "            \"generation_kwargs\": {\n",
        "                \"max_new_tokens\": 2048,\n",
        "                \"temperature\": 0.1\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "FHchLCjQQdwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the new generation parameters with `dry_run`\n",
        "\n",
        "As we're trying new parameters (or if it's the first time executing the pipeline), it's not ideal to execute the pipeline with the whole dataset, as it could fail or the results are not as we expected, wasting money and time.\n",
        "\n",
        "To test that everything works as expected with a small subset of the dataset, we can use the `dry_run` method:"
      ],
      "metadata": {
        "id": "rq4W3QknKzZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distiset = pipeline.dry_run(parameters=parameters)"
      ],
      "metadata": {
        "id": "SyOBQkAAQkiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool! It worked flawlessly! Now that we're sure, let's execute the pipeline again but this time with the entire dataset."
      ],
      "metadata": {
        "id": "dD0UgwmFRe6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distiset = pipeline.run(parameters=parameters, use_cache=False)"
      ],
      "metadata": {
        "id": "EW4Mtq-HRFiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining a few LLMs to generate better responses\n",
        "\n",
        "We can even try to go a step further and use a Mixture-of-Agents (MoA) to combine a few LLMs to try generating richer and better responses.\n",
        "\n",
        "The idea behind MoA is quite simple:\n",
        "\n",
        "1. We have a few LLMs that we will call proposers generating an output for a given input. We will do this certain number of times, providing the previous outputs in the system prompt. This little trick will help the LLM to generate better responses every turn even if the previous outputs are not very good. In order to cover as much fields as possible, it's better to use specialized LLMs as proposers.\n",
        "2. We have a final LLM that we will call aggregator. This aggregator LLM will receive the outputs of the proposers to create and aggregated final output. For the aggregator LLM, we will want to use an LLM that it's proficient at generating text and aggregating the outputs to synthesize a high-quality response.\n",
        "\n",
        "So... what can be a good LLM to be used as an aggregator? ü§î\n",
        "\n",
        "Yes, you guessed it! üéâ `Llama 3.1 405B Instruct`\n",
        "\n",
        "For the proposers LLMs we will use the following models available with [Inference for PROs](https://huggingface.co/blog/inference-pro):\n",
        "\n",
        "- `Code Llama Instruct`: a conversational code assistant. Good at coding üë®üèª‚Äçüíª\n",
        "- `Llama 3.1 70B Instruct`: a good chat model that is good at everything."
      ],
      "metadata": {
        "id": "fT2G5hK3TFz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.llms import InferenceEndpointsLLM, MixtureOfAgentsLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import LoadDataFromHub\n",
        "from distilabel.steps.tasks import TextGeneration, UltraFeedback\n",
        "from distilabel.steps import CombineColumns\n",
        "\n",
        "with Pipeline(name=\"synthetic-data-with-llama3-moa\") as pipeline:\n",
        "    load_dataset = LoadDataFromHub(\n",
        "        repo_id= \"argilla/10Kprompts-mini\"\n",
        "    )\n",
        "\n",
        "    generate = TextGeneration(\n",
        "        llm=MixtureOfAgentsLLM(\n",
        "            proposers_llms=[\n",
        "                InferenceEndpointsLLM(\n",
        "                    model_id=\"codellama/CodeLlama-34b-Instruct-hf\",\n",
        "                    tokenizer_id=\"codellama/CodeLlama-34b-Instruct-hf\",\n",
        "                    generation_kwargs={\n",
        "                        \"max_new_tokens\": 1024,\n",
        "                        \"temperature\": 0.7,\n",
        "                    }\n",
        "                ),\n",
        "                InferenceEndpointsLLM(\n",
        "                    model_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                    tokenizer_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                    generation_kwargs={\n",
        "                        \"max_new_tokens\": 1024,\n",
        "                        \"temperature\": 0.7,\n",
        "                    }\n",
        "                ),\n",
        "            ],\n",
        "            aggregator_llm=InferenceEndpointsLLM(\n",
        "                model_id=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
        "                tokenizer_id=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
        "                generation_kwargs={\n",
        "                    \"max_new_tokens\": 1024,\n",
        "                    \"temperature\": 0.7,\n",
        "                }\n",
        "            )\n",
        "        ),\n",
        "        num_generations=2,\n",
        "        group_generations=True,\n",
        "    )\n",
        "\n",
        "    combine = CombineColumns(\n",
        "      columns=[\"generation\", \"model_name\"],\n",
        "      output_columns=[\"generations\", \"model_names\"]\n",
        "    )\n",
        "\n",
        "    rate = UltraFeedback(aspect=\"overall-rating\", llm=InferenceEndpointsLLM(\n",
        "        model_id=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
        "        tokenizer_id=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
        "        generation_kwargs={\n",
        "            \"max_new_tokens\": 2048,\n",
        "            \"temperature\": 0.0,\n",
        "        }\n",
        "    ))\n",
        "\n",
        "    load_dataset >> generate >> combine >> rate"
      ],
      "metadata": {
        "id": "ln3HXwjKWo3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distiset = pipeline.run(use_cache=False)"
      ],
      "metadata": {
        "id": "Hy8xXN0oa2d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's next?\n",
        "\n",
        "This notebook has scratched the surface of what's possible with the new Llama 3.1 models and distilabel. There's many things to discover and experiment with.\n",
        "\n",
        "This notebook uses Hugging Face Inference Endpoints for PROs. This is good for experimentation. For larger datasets we recommend using local LLMs, TGI, vLLM, and even the upcoming Ray integration for running data generation on GPU clusters.\n",
        "\n",
        "Regarding the pipelines, the best place to discover out-of-the-box components is the [Component Gallery](https://distilabel.argilla.io/latest/components-gallery/).\n",
        "\n",
        "But probably the biggest strength of distilabel is the ability to develop your custom components on top a scalable and robust data generation framework, you can read this [guide to get started](https://distilabel.argilla.io/latest/sections/how_to_guides/basic/step/#define-steps-for-your-pipeline)."
      ],
      "metadata": {
        "id": "ZJ4RdIPYgv5P"
      }
    }
  ]
}